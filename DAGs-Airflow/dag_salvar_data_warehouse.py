# -*- coding: utf-8 -*-
"""DAG-Salvar-Data-Warehouse.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1niMLYwolZv08UX9RngLUk018YUXyl5e9

# DAG para carregar dados em um esquema estrela no PostgreSQL e transferi-los para o BigQuery

Esta DAG executa as seguintes operações:
1. Conecta ao PostgreSQL e cria tabelas em um esquema estrela se não existirem.
2. Insere dados de um DataFrame no esquema estrela do PostgreSQL.
3. Transfere os dados do PostgreSQL para o BigQuery para análise adicional.

Tasks:
- criar_tabelas_postgresql: Conecta ao PostgreSQL e cria as tabelas necessárias para um esquema estrela.
- inserir_dados_postgresql: Insere dados de um DataFrame no esquema estrela do PostgreSQL.
- transferir_dados_bigquery: Transfere dados do PostgreSQL para o BigQuery para análise.
"""

from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from airflow.utils.dates import days_ago
import os
import psycopg2
from datetime import datetime, timedelta
from google.cloud import bigquery
import pandas as pd

# Definindo argumentos padrão
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': days_ago(1),
    'email': ['airflow@example.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

# Inicializando a DAG
dag = DAG(
    'datawarehouse_esquema_estrela',
    default_args=default_args,
    description='DAG para carregar dados em um esquema estrela',
    schedule_interval=None,
)

# Função para conectar ao PostgreSQL
def conectar_postgresql():
    try:
        dbname = os.getenv('POSTGRES_DB')
        user = os.getenv('POSTGRES_USER')
        password = os.getenv('POSTGRES_PASSWORD')
        host = os.getenv('POSTGRES_HOST')
        port = os.getenv('POSTGRES_PORT')

        conn = psycopg2.connect(
            dbname=dbname,
            user=user,
            password=password,
            host=host,
            port=port
        )
        return conn
    except Exception as e:
        print(f'Erro ao conectar ao PostgreSQL: {str(e)}')
        return None

# Função para criar tabelas de um esquema estrela no PostgreSQL
def criar_tabelas(conn):
    try:
        if conn is not None:
            cur = conn.cursor()

            tabelas = {
                "dim_data": """
                    CREATE TABLE IF NOT EXISTS dim_data (
                        id SERIAL PRIMARY KEY,
                        data DATE,
                        dia INT,
                        mes INT,
                        ano INT
                    );
                """,
                "dim_temperatura": """
                    CREATE TABLE IF NOT EXISTS dim_temperatura (
                        id SERIAL PRIMARY KEY,
                        temp_max_dubai FLOAT,
                        temp_min_dubai FLOAT,
                        temp_media_dubai FLOAT
                    );
                """,
                "dim_moeda": """
                    CREATE TABLE IF NOT EXISTS dim_moeda (
                        id SERIAL PRIMARY KEY,
                        usd_aed_open FLOAT,
                        usd_aed_high FLOAT,
                        usd_aed_low FLOAT,
                        usd_aed_close FLOAT,
                        usd_brl_open FLOAT,
                        usd_brl_high FLOAT,
                        usd_brl_low FLOAT,
                        usd_brl_close FLOAT,
                        aed_brl_open FLOAT,
                        aed_brl_high FLOAT,
                        aed_brl_low FLOAT,
                        aed_brl_close FLOAT
                    );
                """,
                "fato_temperatura_moeda": """
                    CREATE TABLE IF NOT EXISTS fato_temperatura_moeda (
                        id SERIAL PRIMARY KEY,
                        data_id INT,
                        temperatura_id INT,
                        moeda_id INT,
                        FOREIGN KEY (data_id) REFERENCES dim_data(id),
                        FOREIGN KEY (temperatura_id) REFERENCES dim_temperatura(id),
                        FOREIGN KEY (moeda_id) REFERENCES dim_moeda(id)
                    );
                """
            }

            for tabela, query in tabelas.items():
                cur.execute(query)
                cur.execute(f"SELECT to_regclass('{tabela}')")
                existe = cur.fetchone()[0]
                if existe:
                    print(f"Tabela {tabela} já existe.")
                else:
                    print(f"Tabela {tabela} foi criada.")

            # Confirmar transações
            conn.commit()

            # Fechar o cursor
            cur.close()
        else:
            print("Não foi possível conectar ao PostgreSQL.")
    except Exception as e:
        print(f'Erro ao criar tabelas: {str(e)}')

# Função para inserir dados no esquema estrela do PostgreSQL
def inserir_dados_esquema_estrela(conn, df):
    try:
        if conn is not None:
            cur = conn.cursor()

            for index, row in df.iterrows():
                # Inserir dados na tabela dim_data
                cur.execute("""
                INSERT INTO dim_data (data, dia, mes, ano)
                VALUES (%s, %s, %s, %s)
                RETURNING id;
                """, (row['Date'], row['Date'].day, row['Date'].month, row['Date'].year))
                data_id = cur.fetchone()[0]

                # Inserir dados na tabela dim_temperatura
                cur.execute("""
                INSERT INTO dim_temperatura (temp_max_dubai, temp_min_dubai, temp_media_dubai)
                VALUES (%s, %s, %s)
                RETURNING id;
                """, (row['Temp_Max_Dubai'], row['Temp_Min_Dubai'], row['Temp_Media_Dubai']))
                temperatura_id = cur.fetchone()[0]

                # Inserir dados na tabela dim_moeda
                cur.execute("""
                INSERT INTO dim_moeda (
                    usd_aed_open, usd_aed_high, usd_aed_low, usd_aed_close,
                    usd_brl_open, usd_brl_high, usd_brl_low, usd_brl_close,
                    aed_brl_open, aed_brl_high, aed_brl_low, aed_brl_close
                )
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                RETURNING id;
                """, (
                    row['USD_AED_Open'], row['USD_AED_High'], row['USD_AED_Low'], row['USD_AED_Close'],
                    row['USD_BRL_Open'], row['USD_BRL_High'], row['USD_BRL_Low'], row['USD_BRL_Close'],
                    None, None, None, None  # Supondo que não há dados para AED/BRL
                ))
                moeda_id = cur.fetchone()[0]

                # Inserir dados na tabela fato_temperatura_moeda
                cur.execute("""
                INSERT INTO fato_temperatura_moeda (data_id, temperatura_id, moeda_id)
                VALUES (%s, %s, %s);
                """, (data_id, temperatura_id, moeda_id))

            # Confirmar transações
            conn.commit()

            # Fechar o cursor
            cur.close()
        else:
            print("Não foi possível conectar ao PostgreSQL.")
    except Exception as e:
        print(f'Erro ao inserir dados no esquema estrela: {str(e)}')

# Função para transferir dados para o BigQuery
def transferir_para_bigquery():
    try:
        # Substitua 'nome_do_dataset' e 'nome_da_tabela' pelos valores reais
        dataset_id = 'nome_do_dataset'
        table_id = 'nome_da_tabela'

        # Carregar o DataFrame de algum lugar (por exemplo, de um arquivo CSV)
        df = pd.read_csv('caminho/do/seu/arquivo.csv')

        # Inicializar o cliente BigQuery
        client = bigquery.Client()

        # Configurar o job de carga para o BigQuery
        job_config = bigquery.LoadJobConfig(
            schema=[
                bigquery.SchemaField('data', 'DATE'),
                bigquery.SchemaField('dia', 'INTEGER'),
                bigquery.SchemaField('mes', 'INTEGER'),
                bigquery.SchemaField('ano', 'INTEGER'),
                bigquery.SchemaField('temp_max_dubai', 'FLOAT'),
                bigquery.SchemaField('temp_min_dubai', 'FLOAT'),
                bigquery.SchemaField('temp_media_dubai', 'FLOAT'),
                bigquery.SchemaField('usd_aed_open', 'FLOAT'),
                bigquery.SchemaField('usd_aed_high', 'FLOAT'),
                bigquery.SchemaField('usd_aed_low', 'FLOAT'),
                bigquery.SchemaField('usd_aed_close', 'FLOAT'),
                bigquery.SchemaField('usd_brl_open', 'FLOAT'),
                bigquery.SchemaField('usd_brl_high', 'FLOAT'),
                bigquery.SchemaField('usd_brl_low', 'FLOAT'),
                bigquery.SchemaField('usd_brl_close', 'FLOAT'),
                bigquery.SchemaField('aed_brl_open', 'FLOAT'),
                bigquery.SchemaField('aed_brl_high', 'FLOAT'),
                bigquery.SchemaField('aed_brl_low', 'FLOAT'),
                bigquery.SchemaField('aed_brl_close', 'FLOAT')
            ],
            write_disposition='WRITE_TRUNCATE',  # Escrever e substituir os dados existentes
        )

        # Carregar os dados para o BigQuery
        job = client.load_table_from_dataframe(
            df, f'{dataset_id}.{table_id}', job_config=job_config
        )

        # Aguardar a conclusão do job
        job.result()

        print(f'Dados carregados com sucesso para o BigQuery: {dataset_id}.{table_id}')

    except Exception as e:
        print(f'Erro ao transferir dados para o BigQuery: {str(e)}')

# Definir as tasks da DAG
with dag:
    # Task para conectar e criar tabelas no PostgreSQL
    criar_tabelas_task = PythonOperator(
        task_id='criar_tabelas_postgresql',
        python_callable=criar_tabelas,
        op_args=[conectar_postgresql()],
    )

    # Task para inserir dados no PostgreSQL
    inserir_dados_task = PythonOperator(
        task_id='inserir_dados_esquema_estrela',
        python_callable=inserir_dados_esquema_estrela,
        op_args=[conectar_postgresql(), df],  # Supondo que 'df' seja o DataFrame a ser inserido
    )

    # Task para transferir dados para o BigQuery
    transferir_bigquery_task = PythonOperator(
        task_id='transferir_para_bigquery',
        python_callable=transferir_para_bigquery,
    )

    # Definir dependências entre as tasks
    criar_tabelas_task >> inserir_dados_task >> transferir_bigquery_task