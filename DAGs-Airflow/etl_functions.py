# -*- coding: utf-8 -*-
"""etl_functions.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19q6csSWv1XfTxQjV9Y-KVM6xpL7KfiKX
"""

import pandas as pd
from google.cloud import storage
import psycopg2
from sqlalchemy import create_engine
import requests
import numpy as np
from datetime import datetime
from meteostat import Point, Daily
from google.cloud import bigquery

def extrair_dados_yahoo_finance(ticker, start_date, end_date):
    import yfinance as yf
    df = yf.download(ticker, start=start_date, end=end_date)
    df.reset_index(inplace=True)
    return df

def limpeza_dados_yahoo_finance(df, prefixo):
    df = df.rename(columns={
        'Date': 'Date',
        'Open': f'{prefixo}_Open',
        'High': f'{prefixo}_High',
        'Low': f'{prefixo}_Low',
        'Close': f'{prefixo}_Close',
        'Adj Close': f'{prefixo}_Adj_Close',
        'Volume': f'{prefixo}_Volume'
    })
    return df

def mesclar_dataframes_por_data(df1, df2):
    df_merged = pd.merge(df1, df2, on='Date', how='inner')
    return df_merged

def extrair_dados_meteostat(latitude, longitude, start_date, end_date):
    start = datetime.strptime(start_date, '%Y-%m-%d')
    end = datetime.strptime(end_date, '%Y-%m-%d')
    location = Point(latitude, longitude)
    data = Daily(location, start, end)
    data = data.fetch()
    data.reset_index(inplace=True)
    return data

def limpeza_e_transformacao(df):
    df = df.rename(columns={
        'time': 'Date',
        'tavg': 'Temp_Media_Dubai',
        'tmin': 'Temp_Min_Dubai',
        'tmax': 'Temp_Max_Dubai'
    })
    df['Temp_Media_Dubai'] = df['Temp_Media_Dubai'] / 10.0
    df['Temp_Min_Dubai'] = df['Temp_Min_Dubai'] / 10.0
    df['Temp_Max_Dubai'] = df['Temp_Max_Dubai'] / 10.0
    return df

def mesclar_e_visualizar(df1, df2):
    df_merged = mesclar_dataframes_por_data(df1, df2)
    return df_merged

def salvar_no_gcs(dataframe, bucket_name, file_name):
    client = storage.Client()
    bucket = client.get_bucket(bucket_name)
    blob = bucket.blob(file_name)
    blob.upload_from_string(dataframe.to_csv(index=False), 'text/csv')

def carregar_dados_do_gcs(bucket_name, file_name):
    client = storage.Client()
    bucket = client.get_bucket(bucket_name)
    blob = bucket.blob(file_name)
    data = blob.download_as_string()
    return pd.read_csv(pd.compat.StringIO(data.decode('utf-8')))

def verificar_informacoes(df):
    print(df.info())

def verificar_consistencia(df):
    valores_ausentes = df.isnull().sum()
    print("Valores ausentes em cada coluna:")
    print(valores_ausentes)
    valores_nulos = df.isna().sum()
    print("\nValores nulos em cada coluna:")
    print(valores_nulos)
    valores_nao_numericos = df.select_dtypes(exclude=[np.number])
    print("\nColunas com valores não numéricos:")
    print(valores_nao_numericos.columns)
    valores_duplicados = df.duplicated().sum()
    print(f"\nNúmero de linhas duplicadas: {valores_duplicados}")
    linhas_por_coluna = df.count()
    print("\nQuantidade de linhas por coluna:")
    print(linhas_por_coluna)
    outliers = df.describe().transpose()
    print("\nVerificação de outliers (estatísticas descritivas):")
    print(outliers)
    var_min = df['Temp_Min_Dubai'].var()
    var_media = df['Temp_Media_Dubai'].var()
    var_max = df['Temp_Max_Dubai'].var()
    print("Variância Temp_Min:", var_min)
    print("Variância Temp_Media:", var_media)
    print("Variância Temp_Max:", var_max)

def analisar_estatisticas(df):
    df['Date'] = pd.to_datetime(df['Date'])
    df['Ano'] = df['Date'].dt.year
    df['Mes'] = df['Date'].dt.month
    df['Dia'] = df['Date'].dt.day
    estatisticas_ano = df.groupby('Ano').describe()
    estatisticas_mes = df.groupby('Mes').describe()
    estatisticas_dia = df.groupby('Dia').describe()
    return estatisticas_ano, estatisticas_mes, estatisticas_dia

def verificar_datas(df):
    df['Date'] = pd.to_datetime(df['Date'])
    min_date = df['Date'].min()
    max_date = df['Date'].max()
    print(f"Data mínima: {min_date}")
    print(f"Data máxima: {max_date}")
    df.set_index('Date', inplace=True)
    return df

def visualizar_distribuicao(df):
    import seaborn as sns
    import matplotlib.pyplot as plt
    sns.histplot(df['USD_AED_Close'], kde=True)
    plt.show()

def visualizar_temperaturas(df):
    import seaborn as sns
    import matplotlib.pyplot as plt
    sns.lineplot(x='Date', y='Temp_Max_Dubai', data=df, label='Temp Max')
    sns.lineplot(x='Date', y='Temp_Media_Dubai', data=df, label='Temp Média')
    sns.lineplot(x='Date', y='Temp_Min_Dubai', data=df, label='Temp Min')
    plt.legend()
    plt.show()

def visualizar_heatmap_mensal(df):
    import seaborn as sns
    import matplotlib.pyplot as plt
    df['Mes'] = df['Date'].dt.month
    df['Ano'] = df['Date'].dt.year
    df_pivot = df.pivot_table(values='USD_AED_Close', index='Ano', columns='Mes', aggfunc='mean')
    sns.heatmap(df_pivot, annot=True, fmt=".2f")
    plt.show()

def visualizar_graficos_linha(df):
    import seaborn as sns
    import matplotlib.pyplot as plt
    sns.lineplot(x='Date', y='USD_AED_Close', data=df)
    plt.show()

def conectar_postgresql():
    conn = psycopg2.connect(
        host="localhost",
        database="meu_banco",
        user="meu_usuario",
        password="minha_senha"
    )
    return conn

def criar_esquema_estrela(conn):
    cursor = conn.cursor()
    cursor.execute("""
    CREATE TABLE IF NOT EXISTS fato (
        id SERIAL PRIMARY KEY,
        Date DATE,
        USD_AED_Close FLOAT,
        USD_AED_High FLOAT,
        USD_AED_Low FLOAT,
        USD_AED_Open FLOAT,
        USD_BRL_Close FLOAT,
        USD_BRL_High FLOAT,
        USD_BRL_Low FLOAT,
        USD_BRL_Open FLOAT,
        Temp_Max_Dubai FLOAT,
        Temp_Min_Dubai FLOAT,
        Temp_Media_Dubai FLOAT
    );
    """)
    conn.commit()
    cursor.close()

def inserir_dados_esquema_estrela(conn, df):
    cursor = conn.cursor()
    for i, row in df.iterrows():
        cursor.execute("""
        INSERT INTO fato (
            Date, USD_AED_Close, USD_AED_High, USD_AED_Low, USD_AED_Open,
            USD_BRL_Close, USD_BRL_High, USD_BRL_Low, USD_BRL_Open,
            Temp_Max_Dubai, Temp_Min_Dubai, Temp_Media_Dubai
        ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
        """, (
            row['Date'], row['USD_AED_Close'], row['USD_AED_High'], row['USD_AED_Low'], row['USD_AED_Open'],
            row['USD_BRL_Close'], row['USD_BRL_High'], row['USD_BRL_Low'], row['USD_BRL_Open'],
            row['Temp_Max_Dubai'], row['Temp_Min_Dubai'], row['Temp_Media_Dubai']
        ))
    conn.commit()
    cursor.close()

def carregar_dados_para_bigquery(dataframe, dataset_name, table_name):
    client = bigquery.Client()
    dataset_ref = client.dataset(dataset_name)
    try:
        client.get_dataset(dataset_ref)
    except Exception as e:
        if 'Not found' in str(e):
            dataset = bigquery.Dataset(dataset_ref)
            dataset = client.create_dataset(dataset)
            print(f'Dataset {dataset.dataset_id} criado no BigQuery.')
    table_id = f"{dataset_name}.{table_name}"
    table_ref = client.dataset(dataset_name).table(table_name)
    try:
        client.get_table(table_ref)
    except Exception as e:
        if 'Not found' in str(e):
            table = bigquery.Table(table_ref)
            table = client.create_table(table)
            print(f'Tabela {table.table_id} criada no BigQuery.')
    job_config = bigquery.LoadJobConfig(
        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,
        create_disposition=bigquery.CreateDisposition.CREATE_IF_NEEDED,
        schema=None
    )
    job = client.load_table_from_dataframe(
        dataframe, table_id, job_config=job_config
    )
    job.result()
    print(f'Dados carregados para a tabela {table_id} no BigQuery com sucesso.')